{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add root project directory to path if needed, since in notebook directory\n",
    "if not any([path.endswith('./../') for path in sys.path]):\n",
    "    sys.path.insert(0, os.path.join(os.getcwd(), './../'))    \n",
    "\n",
    "import requests\n",
    "import io\n",
    "import os\n",
    "import s3fs\n",
    "import glob\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from dask import delayed\n",
    "from distributed import Client\n",
    "from distributed import progress, wait\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from core.data_acquisition import TaxiData\n",
    "\n",
    "clean_df = TaxiData.clean_df\n",
    "\n",
    "# S3 File system\n",
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "# Connection to the dask scheduler\n",
    "client = Client(f'tcp://0.0.0.0:8786')\n",
    "client.restart()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "@delayed\n",
    "def add(x, y):\n",
    "    time.sleep(random.random())\n",
    "    return x + y\n",
    "\n",
    "@delayed\n",
    "def subtract_1(x):\n",
    "    time.sleep(random.random())\n",
    "    return x - 1\n",
    "\n",
    "@delayed\n",
    "def summer(arr):\n",
    "    time.sleep(random.random())\n",
    "    return sum(arr)\n",
    "    \n",
    "sub_results = []\n",
    "for i in range(8):\n",
    "    \n",
    "    add_result = add(i, i*2)\n",
    "    subtract_result = subtract_1(add_result)\n",
    "    sub_results.append(subtract_result)\n",
    "    \n",
    "total = summer(sub_results)\n",
    "    \n",
    "total.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in and Look at Taxi Data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv('s3://dask-data/nyc-taxi/2015/*.csv',\n",
    "                 storage_options={'anon': True})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nothing is actually computed, just meta data sampling, lets persist it to cluster RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['tpep_pickup_datetime'] = dd.to_datetime(df.tpep_pickup_datetime, yearfirst=True, errors='coerce')\n",
    "df['tpep_dropoff_datetime'] = dd.to_datetime(df.tpep_dropoff_datetime, yearfirst=True, errors='coerce')\n",
    "\n",
    "df = client.persist(df)\n",
    "progress(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary stats..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df.count()\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count.compute().VendorID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.total_amount.describe().compute().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many are below 0 and how many are above the 99% quantile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Count of fares below  0: ', df.where(df.total_amount < 0).VendorID.count().compute())\n",
    "_99th_quantile = df.total_amount.quantile(q=0.99).compute()\n",
    "print('99% quantile: ', _99th_quantile)\n",
    "print('Count of fares over  99%: ', df.where(df.total_amount > _99th_quantile).VendorID.count().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's limit the data to fares between 0 and 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.where(df.total_amount.between(0, 70))\n",
    "df.count().compute().VendorID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# A little bit of analysis...\n",
    "\n",
    "## Do more passengers == longer distances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(df.passenger_count).trip_distance.mean().compute()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "grouped.plot.barh()\n",
    "plt.title('Passenger Count vs Mean Distance Traveled')\n",
    "plt.xlabel('Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average speed by hour, maybe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate speed of trip\n",
    "speed = (df.trip_distance / ((df.tpep_dropoff_datetime - df.tpep_pickup_datetime).astype('timedelta64[m]').astype(float) / 60))\n",
    "\n",
    "# Replace inf values with NaNs\n",
    "speed = speed.map(lambda val: val if val not in [np.inf, -np.inf] else np.NaN)\n",
    "\n",
    "# Replace extreme values with NaNs\n",
    "low = speed.quantile(q=0.01).compute()\n",
    "high = speed.quantile(q=0.9).compute()\n",
    "speed = speed.map(lambda val: val if low < val < high and not pd.isnull(val) else np.NaN)\n",
    "\n",
    "# Assign columns\n",
    "df['speed'] = speed\n",
    "df['hour'] = df.tpep_pickup_datetime.dt.hour\n",
    "\n",
    "df = df.persist()\n",
    "progress(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "speed_by_hour = df.groupby('hour').speed.mean().compute()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "speed_by_hour.plot.line()\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Speed')\n",
    "plt.title('Hour of day and average speed')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does faster speed == better tip?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tmp = df.dropna(subset=['speed'])\n",
    "tmp = tmp.persist()\n",
    "tmp['speed_rounded'] = tmp.speed.map(lambda val: int(5 * (float(val) / 5)))\n",
    "tips_by_speed = tmp.groupby('speed_rounded').tip_amount.mean().compute()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "tips_by_speed.plot.line()\n",
    "plt.xlabel('Speed')\n",
    "plt.ylabel('Tip Amount')\n",
    "plt.title('Speed vs Tip Amount')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When are the best trip fractions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[(df.tip_amount > 0) & (df.fare_amount > 0)]    # filter out bad rows\n",
    "df2['tip_fraction'] = df2.tip_amount / df2.fare_amount  # make new column\n",
    "\n",
    "dayofweek = (df2.groupby(df2.tpep_pickup_datetime.dt.dayofweek)\n",
    "                .tip_fraction\n",
    "                .mean()\n",
    "            ).compute()\n",
    "hour      = (df2.groupby(df2.tpep_pickup_datetime.dt.hour)\n",
    "                .tip_fraction\n",
    "                .mean()\n",
    "            ).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "hour.plot.line()\n",
    "plt.ylabel('Tip fraction of Fare')\n",
    "plt.xlabel('Hour')\n",
    "plt.title('Fraction of Tip by Hour')\n",
    "plt.show()\n",
    "\n",
    "print('----- By Day of Week -----')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = dayofweek.plot.line()\n",
    "ax.set_xticklabels(['', 'Mon', 'Tu', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML with Dask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exclude_cols = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'VendorID', 'tip_amount',\n",
    "                'RateCodeID', 'store_and_fwd_flag', 'payment_type']\n",
    "\n",
    "X = dd.read_csv('s3://dask-data/nyc-taxi/2015/*.csv',\n",
    "                storage_options={'anon': True})\n",
    "\n",
    "columns = [col for col in X.columns if col not in exclude_cols]\n",
    "X = X.dropna(subset=[c for c in X.columns if c not in exclude_cols])\n",
    "y = (X.tip_amount > 10).astype(int)\n",
    "\n",
    "X = X[columns]\n",
    "\n",
    "X = X.persist()\n",
    "y = y.persist()\n",
    "\n",
    "progress(X)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly split the data between training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xTest = X.random_split([0.70, 0.30], random_state=1234)\n",
    "yTrain, yTest = y.random_split([0.70, 0.30], random_state=1234)\n",
    "\n",
    "xTrain = xTrain.persist()\n",
    "xTest  = xTest.persist()\n",
    "yTrain = yTrain.persist()\n",
    "yTest  = yTest.persist()\n",
    "progress(xTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from dask_ml.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(max_iter=4)\n",
    "clf.fit(xTrain.values, yTrain.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.score(xTest.values, yTest.values).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InfluxDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv('s3://dask-data/nyc-taxi/2015/yellow_tripdata_2015-11.csv',\n",
    "                 storage_options={'anon': True})\n",
    "df['tpep_pickup_datetime'] = dd.to_datetime(df.tpep_pickup_datetime, yearfirst=True, errors='coerce')\n",
    "df['tpep_dropoff_datetime'] = dd.to_datetime(df.tpep_dropoff_datetime, yearfirst=True, errors='coerce')\n",
    "\n",
    "df = client.persist(df)\n",
    "progress(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install influxdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from influxdb import InfluxDBClient\n",
    "\n",
    "client = InfluxDBClient('localhost', 8086, 'admin', 'admin', 'admin')\n",
    "\n",
    "points = []\n",
    "batch_size = 1000\n",
    "\n",
    "chunk = df.loc[:, ['tpep_dropoff_datetime', 'trip_distance']]\n",
    "\n",
    "for idx, row in chunk.iterrows():\n",
    "    \n",
    "    point = {\n",
    "        \"measurement\": \"distance\",\n",
    "        \"tags\": {\n",
    "            \"year\": 2015,\n",
    "            \"month\": 11\n",
    "        },\n",
    "        \"time\": row['tpep_dropoff_datetime'],\n",
    "        \"fields\": {\n",
    "            \"value\": row['trip_distance']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    points.append(point)\n",
    "    \n",
    "    if len(points) >= batch_size:\n",
    "        client.write_points(points)\n",
    "        points.clear()\n",
    "    \n",
    "client.write_points(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "counts = []\n",
    "\n",
    "# Loop through years 2009-2017\n",
    "for year in np.arange(2009, 2018):\n",
    "    \n",
    "    # Get only files pertaining to this year\n",
    "    files = [f for f in fs.ls('nyc-tlc/trip data/') if str(year) in f and 'yellow' in f]\n",
    "    \n",
    "    # Process files in parallel. (client is asynchronous)\n",
    "    for i, file in enumerate(files):\n",
    "        \n",
    "        # Extract year and month from filename\n",
    "        _year, month = file[-11:-4].split('-')\n",
    "        \n",
    "        # Process data for current year and month\n",
    "        df = dd.read_csv('s3://' + file, \n",
    "                         dtype='object',\n",
    "                         error_bad_lines=False,\n",
    "                         blocksize=int(128e6))\n",
    "        df = df.map_partitions(clean_df)\n",
    "        df = client.persist(df)\n",
    "        \n",
    "        # Yearly dataframe merging\n",
    "        main_df = df if not i else main_df.append(df)\n",
    "    \n",
    "    # Write year's df to S3\n",
    "    main_df.to_csv('s3://milesg-taxi-data-east/yellow-{year}-*.csv.gz'.format(year=year), compression='gzip')\n",
    "    counts.append(main_df.passenger_count.count().compute())\n",
    "    sys.stdout.write('\\rYear: {} - Total {}'.format(_year, sum(counts)))\n",
    "    \n",
    "    # Clear from cluster memory\n",
    "    client.cancel(main_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
