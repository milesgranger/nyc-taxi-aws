{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a list of all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -ls /tmp/parsed-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/tmp/raw-data/yellow_tripdata_2009-01.csv', '/tmp/raw-data/yellow_tripdata_2009-02.csv', '/tmp/raw-data/yellow_tripdata_2009-03.csv', '/tmp/raw-data/yellow_tripdata_2009-04.csv', '/tmp/raw-data/yellow_tripdata_2009-05.csv', '/tmp/raw-data/yellow_tripdata_2009-06.csv', '/tmp/raw-data/yellow_tripdata_2009-07.csv', '/tmp/raw-data/yellow_tripdata_2009-08.csv', '/tmp/raw-data/yellow_tripdata_2009-09.csv', '/tmp/raw-data/yellow_tripdata_2009-10.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql.types import StringType, StructField, StructType\n",
    "\n",
    "sqlc = SQLContext(sc)\n",
    "files = subprocess.check_output('hadoop fs -ls /tmp/raw-data'.split()).strip().split('\\n')\n",
    "files = [file for file in map(lambda path: path.split()[-1], files) if file.endswith('.csv') and 'yellow' in file]\n",
    "print(files[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a list of RDDs, where each is the file in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs:///tmp/raw-data/yellow_tripdata_2009-02.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'vendor_name,Trip_Pickup_DateTime,Trip_Dropoff_DateTime,Passenger_Count,Trip_Distance,Start_Lon,Start_Lat,Rate_Code,store_and_forward,End_Lon,End_Lat,Payment_Type,Fare_Amt,surcharge,mta_tax,Tip_Amt,Tolls_Amt,Total_Amt',\n",
       " u'',\n",
       " u'DDS,2009-02-03 08:25:00,2009-02-03 08:33:39,1,1.6000000000000001,-73.992767999999998,40.758324999999999,,,-73.994709999999998,40.739722999999998,CASH,6.9000000000000004,0,,0,0,6.9000000000000004']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = [sc.textFile('hdfs://' + file) for file in files]\n",
    "data_file = data_files[1]\n",
    "print(data_file.name())\n",
    "data_file.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split each line into fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'vendor_name',\n",
       "  u'Trip_Pickup_DateTime',\n",
       "  u'Trip_Dropoff_DateTime',\n",
       "  u'Passenger_Count',\n",
       "  u'Trip_Distance',\n",
       "  u'Start_Lon',\n",
       "  u'Start_Lat',\n",
       "  u'Rate_Code',\n",
       "  u'store_and_forward',\n",
       "  u'End_Lon',\n",
       "  u'End_Lat',\n",
       "  u'Payment_Type',\n",
       "  u'Fare_Amt',\n",
       "  u'surcharge',\n",
       "  u'mta_tax',\n",
       "  u'Tip_Amt',\n",
       "  u'Tolls_Amt',\n",
       "  u'Total_Amt'],\n",
       " [u'']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = data_file.map(lambda line: line.split(','))\n",
    "data_file.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|Trip_Pickup_DateTime|Passenger_Count|\n",
      "+--------------------+---------------+\n",
      "| 2009-02-03 08:25:00|              1|\n",
      "| 2009-02-28 00:26:00|              5|\n",
      "| 2009-02-22 00:39:23|              1|\n",
      "+--------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType\n",
    "\n",
    "field_names = ' '.join(data_file.take(1)[0])\n",
    "column_count = len(field_names.split(' '))\n",
    "data_file = data_file.filter(lambda line: len(line) == column_count and ' '.join(line) != field_names)\n",
    "\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in field_names.split(' ')]\n",
    "schema = StructType(fields)\n",
    "df = data_file.toDF(schema=schema)\n",
    "\n",
    "df.select(*['Trip_Pickup_DateTime', 'Passenger_Count']).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# header for 07-12 months in 2016 require a  pre-defined column\n",
    "bad_2016_cols = ['vendorid',\n",
    "                 'tpep_pickup_datetime',\n",
    "                 'tpep_dropoff_datetime',\n",
    "                 'passenger_count',\n",
    "                 'trip_distance',\n",
    "                 'pickup_longitude',\n",
    "                 'pickup_latitude',\n",
    "                 'ratecodeid',\n",
    "                 'store_and_fwd_flag',\n",
    "                 'dropoff_longitude',\n",
    "                 'dropoff_latitude',\n",
    "                 'payment_type',\n",
    "                 'fare_amount',\n",
    "                 'extra',\n",
    "                 'mta_tax',\n",
    "                 'tip_amount',\n",
    "                 'tolls_amount',\n",
    "                 'improvement_surcharge',\n",
    "                 'total_amount']\n",
    "\n",
    "# Final columns we're interested in\n",
    "columns = [\n",
    "            'pickup_datetime',\n",
    "            'dropoff_datetime',\n",
    "            'tip_amount',\n",
    "            'fare_amount',\n",
    "            'total_amount',\n",
    "            'vendor_id',\n",
    "            'passenger_count',\n",
    "            'trip_distance',\n",
    "            'payment_type',\n",
    "            'tolls_amount',\n",
    "        ]\n",
    "\n",
    "# Some files require some hard renaming of column names\n",
    "hard_renames = {\n",
    "    'vendor_name': 'vendor_id',\n",
    "    'total_amt': 'total_amount',\n",
    "    'tolls_amt': 'tolls_amount',\n",
    "    'fare_amt': 'fare_amount',\n",
    "    'tip_amt': 'tip_amount',\n",
    "    'trip_pickup_datetime': 'pickup_datetime',\n",
    "    'trip_dropoff_datetime': 'dropoff_datetime'          \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all into cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: 2017 Month: 06()\n",
      "['pickup_datetime', 'dropoff_datetime', 'tip_amount', 'fare_amount', 'total_amount', 'vendor_id', 'passenger_count', 'trip_distance', 'payment_type', 'tolls_amount']\n",
      "+-------------------+-------------------+----------+\n",
      "|    pickup_datetime|   dropoff_datetime|tip_amount|\n",
      "+-------------------+-------------------+----------+\n",
      "|2017-06-08 07:52:31|2017-06-08 08:01:32|      1.86|\n",
      "|2017-06-08 08:08:18|2017-06-08 08:14:00|      2.34|\n",
      "|2017-06-08 08:16:49|2017-06-08 15:43:22|         0|\n",
      "+-------------------+-------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "for i, data_file in enumerate(data_files):\n",
    "    \n",
    "    year, month = data_file.name()[-11:-4].split('-')\n",
    "    sys.stdout.write('\\rYear: {} Month: {}'.format(year, month))\n",
    "    \n",
    "    # Split data_file lines into fields\n",
    "    data_file = data_file.map(lambda line: line.split(','))\n",
    "    \n",
    "    # Define field_names, column_count varies if this is late 2016\n",
    "    field_names = ' '.join([c.strip() for c in data_file.take(1)[0] if c.strip()])\n",
    "    if year != '2016' and month not in ['07', '08', '09', '10', '11', '12']:\n",
    "        column_count = len(field_names.split(' ')) \n",
    "    else:\n",
    "        column_count = len(bad_2016_cols)\n",
    "        \n",
    "    # Remove the header and any row which doesn't match length of column_count / header\n",
    "    data_file = data_file.filter(lambda line: len(line) == column_count and ' '.join(line) != field_names)\n",
    "    \n",
    "    # check to see if field_names need to be changed, needed above to kick out wrong header\n",
    "    if year == '2016' and month in ['07', '08', '09', '10', '11', '12']:\n",
    "        field_names = ' '.join(bad_2016_cols)\n",
    "\n",
    "    # Construct schema and convert to dataframe\n",
    "    fields = [StructField(field_name, StringType(), True) for field_name in field_names.split(' ')]\n",
    "    schema = StructType(fields)\n",
    "    df = sqlc.createDataFrame(data_file, schema=schema)\n",
    "    \n",
    "    # Rename all columns to .lower() and hard renaming mapping\n",
    "    df = df.select([col(col_name).alias(col_name.lower() \n",
    "                                    if col_name.lower() not in hard_renames.keys() else \n",
    "                                    hard_renames[col_name.lower()]\n",
    "                                   )\n",
    "                for col_name in df.columns]\n",
    "              )\n",
    "    \n",
    "    # Rename all columns to not have _ or 'tpep'\n",
    "    df = df.select([col(col_name).alias(col_name.replace('_', '').replace('tpep', '')) for col_name in df.columns])\n",
    "    \n",
    "    # Now select all columns in dataframe which match columns mapping\n",
    "    df = df.select([col(col_name.replace('_', '')).alias(col_name) for col_name in columns])\n",
    "    \n",
    "    if not i:\n",
    "        final_df = df\n",
    "    else:\n",
    "        final_df = final_df.unionAll(df)\n",
    "\n",
    "print()\n",
    "print(df.columns)\n",
    "df.select(*columns[:3]).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.write.save('hdfs:///tmp/parsed-data/data', format='csv', header=True, sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
